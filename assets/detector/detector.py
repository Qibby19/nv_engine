import tkinter as tk
from tkinter import filedialog


import os
import sys
import time
import json
import hashlib
import threading
import requests
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import argparse


try:
    import yara
    YARA_AVAILABLE = True
except ImportError:
    YARA_AVAILABLE = False
    print("Warning: YARA not installed. Install with: pip install yara-python")


class OllamaClient:
    """Client for interacting with Ollama API"""
   
    def __init__(self, base_url="http://localhost:11434", model="llama3.2"):
        self.base_url = base_url.rstrip('/')
        self.model = model
        self.session = requests.Session()
       
        # Test connection
        if not self.test_connection():
            print(f" Warning: Cannot connect to Ollama at {base_url}")
            print("Make sure Ollama is running with: ollama serve")
   
    def test_connection(self):
        """Test if Ollama is running and accessible"""
        try:
            response = self.session.get(f"{self.base_url}/api/tags", )
            return response.status_code == 200
        except:
            return False
   
    def analyze_code(self, file_content, file_path, context=""):
        """Send code to Ollama for malware analysis"""
       
        # Craft a specific prompt for malware analysis
        prompt = f"""
You are a cybersecurity expert analyzing code for potential malware. Analyze the following file for:


1. MALICIOUS BEHAVIOR: Look for suspicious activities like:
   - System command execution
   - Network communications
   - File system manipulation
   - Registry modifications
   - Process injection
   - Data exfiltration
   - Obfuscation techniques


2. AI-GENERATED PATTERNS: Look for signs the code was generated by AI:
   - Excessive commenting
   - Generic variable/function names
   - Repetitive patterns
   - Over-engineered structure
   - Typical AI coding style


3. THREAT ASSESSMENT: Rate the threat level (LOW/MEDIUM/HIGH/CRITICAL)


File: {file_path}
Context: {context}


--- CODE START ---
{file_content[:4000]}  # Limit to first 4000 chars
--- CODE END ---


Provide a structured analysis in this format:
THREAT_LEVEL: [LOW/MEDIUM/HIGH/CRITICAL]
MALICIOUS_INDICATORS: [list specific suspicious behaviors found]
AI_GENERATED: [YES/NO - with confidence percentage]
EXPLANATION: [detailed technical analysis]
RECOMMENDATION: [what action to take]
"""


        try:
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,  # Low temperature for consistent analysis
                    "top_p": 0.9,
                    "num_predict": 500   # Limit response length
                }
            }
           
            print(f" Sending to Ollama ({self.model}) for analysis...")
           
            response = self.session.post(
                f"{self.base_url}/api/generate",
                json=payload,
               
            )
           
            if response.status_code == 200:
                result = response.json()
                return self.parse_ollama_response(result.get('response', ''))
            else:
                return {
                    'error': f"Ollama API error: {response.status_code}",
                    'threat_level': 'UNKNOWN',
                    'ai_generated': 'UNKNOWN',
                    'explanation': 'Failed to analyze with Ollama'
                }
               
        except requests.exceptions.Timeout:
            return {
                'error': 'Ollama request error',
                'threat_level': 'UNKNOWN',
                'ai_generated': 'UNKNOWN',
                'explanation': 'Analysis timed out'
            }
        except Exception as e:
            return {
                'error': str(e),
                'threat_level': 'UNKNOWN',
                'ai_generated': 'UNKNOWN',
                'explanation': f'Error during analysis: {str(e)}'
            }
   
    def parse_ollama_response(self, response_text):
        """Parse structured response from Ollama"""
        result = {
            'threat_level': 'UNKNOWN',
            'malicious_indicators': [],
            'ai_generated': 'UNKNOWN',
            'explanation': '',
            'recommendation': '',
            'raw_response': response_text
        }
       
        lines = response_text.split('\n')
        current_section = None
       
        for line in lines:
            line = line.strip()
           
            if line.startswith('THREAT_LEVEL:'):
                result['threat_level'] = line.split(':', 1)[1].strip()
            elif line.startswith('MALICIOUS_INDICATORS:'):
                indicators = line.split(':', 1)[1].strip()
                result['malicious_indicators'] = [i.strip() for i in indicators.split(',') if i.strip()]
            elif line.startswith('AI_GENERATED:'):
                result['ai_generated'] = line.split(':', 1)[1].strip()
            elif line.startswith('EXPLANATION:'):
                result['explanation'] = line.split(':', 1)[1].strip()
                current_section = 'explanation'
            elif line.startswith('RECOMMENDATION:'):
                result['recommendation'] = line.split(':', 1)[1].strip()
                current_section = 'recommendation'
            elif current_section == 'explanation' and line:
                result['explanation'] += ' ' + line
            elif current_section == 'recommendation' and line:
                result['recommendation'] += ' ' + line
       
        return result
   
class EnhancedYARADetector:
    """Enhanced YARA detector with additional rules"""
   
    def __init__(self):
        self.rules_content = '''
        rule AI_Generated_Malware {
            meta:
                description = "Detects AI-generated malware patterns"
                author = "AI Malware Detector"
            strings:
                $ai1 = "This is a comment explaining"
                $ai2 = "Function to process"
                $ai3 = "Method to gather"
                $ai4 = "class SystemInformationGatherer"
                $ai5 = "class FileSystemManager"
                $ai6 = "class NetworkCommunicator"
                $ai7 = "[INFO]"
                $ai8 = "[ERROR]"
                $ai9 = "[STAGE"
                $ai10 = "def collect_"
                $ai11 = "def generate_"
                $ai12 = "def simulate_"
            condition:
                3 of ($ai*)
        }
       
        rule Suspicious_Commands {
            meta:
                description = "Detects suspicious system commands"
            strings:
                $cmd1 = "cmd.exe" nocase
                $cmd2 = "powershell" nocase
                $cmd3 = "rundll32" nocase
                $cmd4 = "regsvr32" nocase
                $cmd5 = "wscript" nocase
                $cmd6 = "cscript" nocase
                $sys1 = "system(" nocase
                $sys2 = "subprocess.call"
                $sys3 = "subprocess.run"
                $sys4 = "shell_exec" nocase
            condition:
                any of them
        }
       
        rule Network_Activity {
            meta:
                description = "Detects network-related malware behavior"
            strings:
                $net1 = "socket.socket"
                $net2 = "urllib.request"
                $net3 = "requests.get"
                $net4 = "requests.post"
                $net5 = "http://"
                $net6 = "https://"
                $net7 = "tcp://"
                $net8 = "ftp://"
            condition:
                2 of them
        }
       
        rule Code_Injection {
            meta:
                description = "Detects code injection techniques"
            strings:
                $inj1 = "VirtualAlloc" nocase
                $inj2 = "WriteProcessMemory" nocase
                $inj3 = "CreateRemoteThread" nocase
                $inj4 = "SetWindowsHookEx" nocase
                $inj5 = "LoadLibrary" nocase
                $inj6 = "GetProcAddress" nocase
                $exec1 = "exec("
                $exec2 = "eval("
            condition:
                any of them
        }
       
        rule Obfuscation_Techniques {
            meta:
                description = "Detects code obfuscation"
            strings:
                $obf1 = "base64.b64decode"
                $obf2 = "base64.b64encode"
                $obf3 = /\\\\x[0-9a-fA-F]{2}/
                $obf4 = /[A-Za-z0-9+\\/]{50,}/
                $obf5 = "chr("
                $obf6 = "ord("
            condition:
                2 of them
        }
        '''
       
        self.rules = None
        if YARA_AVAILABLE:
            try:
                self.rules = yara.compile(source=self.rules_content)
                print(" YARA rules compiled successfully")
            except Exception as e:
                print(f" Error compiling YARA rules: {e}")
                self.rules = None
   
    def scan(self, file_path):
        """Scan file with YARA rules"""
        if not self.rules:
            return []
       
        try:
            matches = self.rules.match(file_path)
            return [{
                'rule': match.rule,
                'meta': dict(match.meta),
                'strings': [(s.identifier, s.instances) for s in match.strings]
            } for match in matches]
        except Exception as e:
            return [{'error': str(e)}]


class RealTimeMalwareDetector:
    """Main real-time malware detector with AI integration"""
   
    def __init__(self, ollama_model="llama3.2", ollama_url="http://localhost:11434"):
        self.ollama_client = OllamaClient(ollama_url, ollama_model)
        self.yara_detector = EnhancedYARADetector()
        self.scan_queue = []
        self.scan_lock = threading.Lock()
        self.stats = defaultdict(int)
        self.scanning = True
       
        # File types to scan
        self.scannable_extensions = {
            '.py', '.js', '.php', '.pl', '.rb', '.sh', '.bat', '.cmd',
            '.ps1', '.vbs', '.jar', '.exe', '.dll', '.scr', '.com',
            '.html', '.htm', '.asp', '.aspx', '.jsp'
        }
       
        # Start background scanner thread
        self.scanner_thread = threading.Thread(target=self._background_scanner, daemon=True)
        self.scanner_thread.start()
   
    def is_scannable_file(self, file_path):
        """Check if file should be scanned"""
        path = Path(file_path)
       
        # Check extension
        if path.suffix.lower() not in self.scannable_extensions:
            return False
       
        # Check file size (skip very large files)
        try:
            if path.stat().st_size > 5 * 1024 * 1024:  # 5MB limit
                return False
        except:
            return False
       
        return True
   
    def queue_file_for_scan(self, file_path, event_type="manual"):
        """Add file to scan queue"""
        if self.is_scannable_file(file_path):
            with self.scan_lock:
                self.scan_queue.append({
                    'file_path': file_path,
                    'event_type': event_type,
                    'timestamp': datetime.now()
                })
   
    def scan_file_comprehensive(self, file_path, event_type="manual"):
        """Comprehensive file analysis combining YARA and AI"""
       
        result = {
            'file_path': str(file_path),
            'scan_timestamp': datetime.now().isoformat(),
            'event_type': event_type,
            'file_size': 0,
            'file_hash': '',
            'yara_matches': [],
            'ai_analysis': {},
            'final_verdict': 'CLEAN',
            'confidence': 0.0,
            'recommendations': []
        }
       
        try:
            # Get file info
            path_obj = Path(file_path)
            result['file_size'] = path_obj.stat().st_size
           
            # Calculate file hash
            with open(file_path, 'rb') as f:
                content = f.read()
                result['file_hash'] = hashlib.sha256(content).hexdigest()[:16]
           
            print(f"\n{'='*60}")
            print(f" ANALYZING: {file_path}")
            print(f" Size: {result['file_size']} bytes | Hash: {result['file_hash']}")
            print(f"â° Event: {event_type} | Time: {datetime.now().strftime('%H:%M:%S')}")
            print(f"{'='*60}")
           
            # YARA Analysis
            print(" Running YARA analysis...")
            yara_matches = self.yara_detector.scan(file_path)
            result['yara_matches'] = yara_matches
           
            yara_threat_level = 0
            if yara_matches and not any('error' in match for match in yara_matches):
                for match in yara_matches:
                    if match.get('rule') in ['Suspicious_Commands', 'Code_Injection']:
                        yara_threat_level = max(yara_threat_level, 3)  # HIGH
                    elif match.get('rule') in ['Network_Activity', 'Obfuscation_Techniques']:
                        yara_threat_level = max(yara_threat_level, 2)  # MEDIUM
                    elif match.get('rule') == 'AI_Generated_Malware':
                        yara_threat_level = max(yara_threat_level, 1)  # LOW-MEDIUM
               
                print(f" YARA MATCHES FOUND:")
                for match in yara_matches:
                    if 'rule' in match:
                        print(f"   - {match['rule']}: {match.get('meta', {}).get('description', 'No description')}")
            else:
                print(" No YARA matches found")
           
            # AI Analysis for suspicious or unknown files
            need_ai_analysis = (
                yara_threat_level > 0 or  # YARA found something
                event_type in ['created', 'modified'] or  # New/changed files
                path_obj.suffix.lower() in ['.py', '.js', '.php', '.ps1']  # Script files
            )
           
            if need_ai_analysis:
                print(" Requesting AI analysis...")
               
                # Read file content for AI analysis
                try:
                    file_content = content.decode('utf-8', errors='ignore')
                except:
                    file_content = str(content)[:1000]  # Fallback for binary files
               
                context = f"YARA matches: {[m.get('rule') for m in yara_matches if 'rule' in m]}"
                ai_result = self.ollama_client.analyze_code(file_content, file_path, context)
                result['ai_analysis'] = ai_result
               
                if 'error' not in ai_result:
                    print(f" AI ANALYSIS:")
                    print(f"   Threat Level: {ai_result.get('threat_level', 'UNKNOWN')}")
                    print(f"   AI Generated: {ai_result.get('ai_generated', 'UNKNOWN')}")
                    if ai_result.get('malicious_indicators'):
                        print(f"   Malicious Indicators: {', '.join(ai_result['malicious_indicators'][:3])}")
                    if ai_result.get('explanation'):
                        print(f"   Analysis: {ai_result['explanation'][:100]}...")
                else:
                    print(f" AI Analysis failed: {ai_result.get('error', 'Unknown error')}")
           
            # Determine final verdict
            final_verdict, confidence = self._calculate_final_verdict(yara_matches, result.get('ai_analysis', {}))
            result['final_verdict'] = final_verdict
            result['confidence'] = confidence
           
            # Generate recommendations
            result['recommendations'] = self._generate_recommendations(result)
           
            # Print final assessment
            self._print_final_assessment(result)
           
            # Update statistics
            self.stats[f'scanned_{event_type}'] += 1
            self.stats[f'verdict_{final_verdict.lower()}'] += 1
           
            return confidence
           
        except Exception as e:
            print(f" Error analyzing {file_path}: {e}")
            result['error'] = str(e)
            result['final_verdict'] = 'ERROR'
            return result
   
    def _calculate_final_verdict(self, yara_matches, ai_analysis):
        """Calculate final threat verdict and confidence"""
       
        yara_score = 0
        ai_score = 0
       
        # YARA scoring
        if yara_matches:
            for match in yara_matches:
                if 'error' not in match:
                    rule = match.get('rule', '')
                    if rule in ['Suspicious_Commands', 'Code_Injection']:
                        yara_score = max(yara_score, 0.8)
                    elif rule in ['Network_Activity', 'Obfuscation_Techniques']:
                        yara_score = max(yara_score, 0.6)
                    elif rule == 'AI_Generated_Malware':
                        yara_score = max(yara_score, 0.4)
       
        # AI scoring
        if ai_analysis and 'error' not in ai_analysis:
            threat_level = ai_analysis.get('threat_level', '').upper()
            if threat_level == 'CRITICAL':
                ai_score = 0.9
            elif threat_level == 'HIGH':
                ai_score = 0.7
            elif threat_level == 'MEDIUM':
                ai_score = 0.5
            elif threat_level == 'LOW':
                ai_score = 0.3
       
        # Combine scores
        combined_score = max(yara_score, ai_score)
       
        # Determine verdict
        if combined_score >= 0.7:
            return "MALICIOUS", combined_score
        elif combined_score >= 0.5:
            return "SUSPICIOUS", combined_score
        elif combined_score >= 0.3:
            return "QUESTIONABLE", combined_score
        else:
            return "CLEAN", combined_score
   
    def _generate_recommendations(self, result):
        """Generate actionable recommendations"""
        recommendations = []
        verdict = result['final_verdict']
       
        if verdict == "MALICIOUS":
            recommendations.extend([
                " QUARANTINE FILE IMMEDIATELY",
                " Block file execution",
                " Investigate source and propagation",
                " Report to security team"
            ])
        elif verdict == "SUSPICIOUS":
            recommendations.extend([
                " Monitor file activity",
                " Run additional scans",
                " Restrict execution permissions",
                " Review code manually"
            ])
        elif verdict == "QUESTIONABLE":
            recommendations.extend([
                " Keep under observation",
                " Monitor for behavior changes",
                " Rescan periodically"
            ])
       
        return recommendations
   
    def _print_final_assessment(self, result):
        """Print comprehensive final assessment"""
        verdict = result['final_verdict']
        confidence = result['confidence']
       
        # Color coding
        colors = {
            'MALICIOUS': '',
            'SUSPICIOUS': '',
            'QUESTIONABLE': '',
            'CLEAN': '',
            'ERROR': ''
        }
       
        color = colors.get(verdict, '')
       
        print(f"\n{'-'*60}")
        print(f" FINAL ASSESSMENT")
        print(f"{'-'*60}")
        print(f"{color} VERDICT: {verdict} (Confidence: {confidence:.1%})")
       
        if result.get('recommendations'):
            print(f" RECOMMENDATIONS:")
            for rec in result['recommendations'][:3]:
                print(f"   {rec}")
       
        print(f"{'-'*60}")
   
    def _background_scanner(self):
        """Background thread for processing scan queue"""
        while self.scanning:
            if self.scan_queue:
                with self.scan_lock:
                    if self.scan_queue:
                        scan_item = self.scan_queue.pop(0)
                    else:
                        scan_item = None
               
                if scan_item:
                    self.scan_file_comprehensive(
                        scan_item['file_path'],
                        scan_item['event_type']
                    )
            else:
                time.sleep(1)
   
    def print_statistics(self):
        """Print scanning statistics"""
        print(f"\n{'='*50}")
        print(f" DETECTION STATISTICS")
        print(f"{'='*50}")
        for key, value in sorted(self.stats.items()):
            print(f"{key.replace('_', ' ').title()}: {value}")
        print(f"{'='*50}")


def main():
    root = tk.Tk()
    root.withdraw()  # Hide the main window


    file_path = filedialog.askopenfilename()  # Open file picker
    parser = argparse.ArgumentParser(description='AI-Powered Real-Time Malware Detector')
    parser.add_argument('--model', default='llama3.2', help='Ollama model to use')
    parser.add_argument('--ollama-url', default='http://localhost:11434', help='Ollama server URL')
   
    args = parser.parse_args()


    if not os.path.exists(file_path):
        print(f" Path not found: {file_path}")
        sys.exit(1)


    detector = RealTimeMalwareDetector(args.model, args.ollama_url)

    if os.path.isfile(file_path):
            # Single file scan
            print(" Scanning single file...")
            confidence = detector.scan_file_comprehensive(file_path, "manual")
    else:
            # Directory scanning
        if args.scan_existing:
            print(" Scanning existing files...")
        for root, dirs, files in os.walk(file_path):
            for file in files:
                file_path = os.path.join(root, file)
                if detector.is_scannable_file(file_path):
                    confidence = detector.queue_file_for_scan(file_path, "existing")


    detector.print_statistics()
    print("\n Detection session completed")
    home_dir = os.path.expanduser("~")
    documents_path = Path(home_dir) / "Documents"
    absolute_documents_path = str(documents_path.resolve())


    print(absolute_documents_path)
    text_path = Path(absolute_documents_path) / "result.txt"


    print(text_path)
   
    file = open(text_path, "w")
    file.write(file_path + "\n")
    file.write(str(confidence) + "\n")
    file.close()


if __name__ == "__main__":
    main()